Team:
1- Octavian Suciu
2- Sanghyun Hong
3-

Email: osuciu@cs.umd.edu, shhong@cs.umd.edu

---------------------------------------------------------------------------------
Part 1)

1- baseline accuracy = training-micro=52%, training-macro=11%, dev-micro=55%, dev-macro=11%
2- Cohenâ€™s Kappa = 0.93

---------------------------------------------------------------------------------
Part 2)
1-

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s)| 11970| 13297      | 12882     | 15085 | 80666   | 15694


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,time)|   12 |       13   |        16 |    15 |      43 |   19

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,loss)|    1 |        0   |         0 |     2 |      23 |    0


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
c(s,export)|    0 |        0   |         0 |     1 |       3 |    0



2-

s   | cord       | division   | formation  | phone      | product    | text
---------------------------------------------------------------------------------
p(s)| 0.09407305 | 0.08718125 | 0.08649207 | 0.10751206 | 0.52584425 | 0.09889731


s        | cord       | division   | formation  | phone      | product    | text
--------------------------------------------------------------------------------------
p(time|s)| 0.16671639 | 0.16335602 | 0.20711042 | 0.16697089 | 0.09235520 | 0.20349109

s        | cord       | division   | formation  | phone      | product    | text
--------------------------------------------------------------------------------------
p(loss|s)| 0.16377016 | 0.00569859 | 0.00587364 | 0.25755663 | 0.56223929 | 0.00486170


s          | cord       | division   | formation  | phone      | product    | text
----------------------------------------------------------------------------------------
p(export|s)| 0.02759985 | 0.02496965 | 0.02573668 | 0.57533558 | 0.32505559 | 0.02130264


3- for the sentence X = "and i can tell you that i 'm an absolute nervous wreck every time she performs . i have her practice the last two lines on each page , so I can learn exactly when to turn the page -- just one of the tricks to this trade that i 've learned the hard way ."

s      | cord              | division           | formation          | phone              | product            | text
-----------------------------------------------------------------------------------------------------------------------------------
p(s|X) | 4.14269066671e-170 | 5.09552900661e-174 | 1.7858117751e-173 | 7.21250170673e-174 | 2.58742119007e-185 | 3.18158665783e-164

4- classifier f-measures on the test set:
micro averaged = 86.70
macro averaged = 78.56

5- We blend smoothing and log-likelihood techniques in our algorithm implementation. Staring from the add-one smoothing (alpha = 1), we varies the alpha from 0.001 to 10.0 increasing in each 0.002 values in order to find the best smoothing values, which is 0.04. With the smoothing, log-likelihood is used to compute the muliplication of probabilities, since several conditional probabilities are really small, so it couldn't be computed unless log values are used. In here, we use the natual log denoted ln(x).

---------------------------------------------------------------------------------
Part 3)

1- .:1.0, ``:1.0, her:2.0, with:1.0, plucky:1.0, an:1.0, to:2.0, plank:1.0, jean-jacques:1.0, last:1.0, by:1.0, line:1.0, tied:1.0, painting:1.0, sits:1.0, drawing:1.0, another:1.0, the:2.0, much-quoted:1.0, not:1.0, shows:1.0, friend:1.0, lady:1.0, a:1.0, madame:1.0, room:1.0, before:1.0, ,:3.0, dog:1.0, in:1.0, down:1.0, managed:1.0, did:1.0, french:1.0, little:1.0, rolland:1.0, !:1.0, exquisite:1.0, harp:1.0, pet:1.0, ah:1.0, liberty:1.0, she:1.0, and:1.0, of:1.0, hauer:1.0, lafayette:1.0, who:1.0, wordsworth:1.0
2- comma separated accuracies (e.g. 30,35,60): 8,8,8,9,9,9,9,13,13,16,20,20,20,25,25,29,33,34,34,39,37,36,36,39,39,39,39,40,43
3- classifier f-measures on the test set:
micro averaged = 81%
macro averaged = 69%
4- The implementation is the averaged version of the algorithm. For each meta-iteration, it shuffles the training set and iterates over all the instances. The number of meta-iterations m = min(MAX_META_ITERATIONS,t: there are no mistakes on entire training set at meta-iteration t). We decided not to use any more advanced heuristics (e.g. based on the dev set performance) because this simple approach converges quickly (0 mistakes after 20 meta-iterations with 2,787 total mistakes). The feature set contains only words extracted from the training set.
---------------------------------------------------------------------------------
Part 4)
A) Feature A:

1- Description - This feature is a random value added to each instance.

2- naive-bayes f-measures on the test set:
micro averaged = 86.20%
macro averaged = 78.56%

3- perceptron f-measures on the test set:
micro averaged = 81%
macro averaged = 70%


4- Conclusions: The random feature does not affect perceptron. The number of updates decreased slightly and the reported f-measure change is not significant. This result highlights perceptron's resilience to noise in the data. For the Naive Bayes classifier the randome floating number does not affect the classification performance. It's because all the random floating numbers are generated by the numpy module has distinct values, and each value is unique in the entire bag of words. This will result just multiplying equal probabilities to all the likelihood, which can't affect the classification result.

B) Feature B:

1- Description - This feature encodes the total number of distinct words for each instance.

2- naive-bayes f-measures on the test set:
micro averaged = 85.39%
macro averaged = 77.11%

3- perceptron f-measures on the test set:
micro averaged = 78%
macro averaged = 66%


4- Conclusions: The feature seems to actually hurt the performance of perceptron. The algorithm hits a hard stop after 30 meta-iterations, with a total of 26,877 updates (10 times the number of mistakes in the non-extended version). One possible explanation is non-stationarity in data: the bow features are only extracted from the training set; perceptron learns some correlation between bow + word_count and the classes. On the test set, word_count comes from a distribution similar to the training set one, but bow represenations are different (words unseen in the training set are just discarded), causing lots of mispredictions. In the case of Naive Bayes classifier, adding the word count in each line of text will have an effect to the classification performance. It's because some of the lines in the datasets can have the same number of words, however, it's definitely not related to the labels. Thus, each likelihood is multiplied by the probabilities not related to the labels, so it will decrease the performance.


